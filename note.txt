import math


def fact(n):
	res = 1
	for i in range(2,n+1):
		res *= i
	return res

def ncr(n, r):
	return fact(n)/(fact(n-r)*fact(r))

bn = []
bn.append(1.0)

for i in range(0,50):
	sum = 0.0
	for k in range(len(bn)):
		sum += bn[k]*ncr(len(bn)+1, k)
	b = (0-sum)/(len(bn)+1)
	if abs(b) < 1.0e-08:
		b = 0
	bn.append(b)

for i in range(0, len(bn), 2):
	print (bn[i])

---------------
-----------------------------------

 n^x = (1 + xlogn/k)^k
 = 1 + (k,1) (xlogn/k) + (k,2) (xlogn/k)^2 + (k,3) (xlogn/k)^3 + ... + (xlogn/k)^k

 sum (n:2->c) ((logn)^k) = c ( (logc)^k - k(logc)^k-1 + k(k-1)(logc)^k-2 + ... + k!)

 sum (n:2->c) (n^x) = 

 c (1 + (k,m) (x/k)^k * ( (logc)^k - k(logc)^k-1 + k(k-1)(logc)^k-2  ) )

 c (1 + (k,m) (x/k)^k * (logc)^k (1 - k(logc)^-1 ) )

 c (1 + axlogc/k)^k   ;   (1 - k(logc)^-1) = a^k

 a^k = (1 - k(logc)^-1)
 
 a = (1 - k(logc)^-1)^(1/k)
 
--------------------------------
 
 c (1 + axlogc/k)^k = -1
 
 (1 + axlogc/k)^k = -1/c

 axlogc/k = (-1/c)^(1/k)
 
---------------

import math

logc = 4000

# logc is the log of the number of terms (c) in the zeta function sum.
# n is the rising integer base for n^x or n^-x in the sum.
# n = 2 to c (excluding the first term of 1)
# n^x is written as e^(xlogn) and approximated as (1 + xlogn/k)^k. 
# k is the finite exponent of e approximation. 
# k is also the number of zeros (or minus ones).
# m is the rising exponent of (xlogn/k) in the binomial expansion of (1 + xlogn/k)^k. 
# m = 1 to k (excluding the first term of 1)

def logsum():
	for m in range (3, 5000, 200):
		sum = 0.0
		v = 1
		sum = 1
		for i in range (m):
			v *= (m-i) / logc
			a = (-1)**(i+1) * v
			sum += a
#		print (sum + math.sqrt(math.log(m)))
		s = (1 - (1/math.e) * math.log (1+ 2.9*m/logc) )
		print (1 + sum, ',', 1 + s)

logsum()

# 1/2 logc + c (logc)^n (2 - 1/e log (1+3n/logc) )

# s = 0.5*gc + c * (gc**n) *(2 - (1/e) * math.log (1+ 3*n/gc) )

# sum = 3 - math.sqrt(math.log(m))
	
	
# x((logx)^1 - 1)
# x((logx)^2 - 2*(logx) + 2*1)
# x((logx)^3 - 3*(logx)^2 + 3*2*(logx) - 3*2*1)
# x((logx)^4 - 4*(logx)^3 + 4*3*(logx)^2 - 4*3*2*(logx) + 4*3*2*1)

# x(logx)^4 ( 1 - 4/(logx) + 4*3/(logx)^2 - 4*3*2/(logx)^3 + 4*3*2*1/(logx)^4)
# x(logx)^3 ( 1 - 3/(logx) + 3*2/(logx)^2 - 3*2*1/(logx)^3)
# x(logx)^2 ( 1 - 2/(logx) + 2*1/(logx)^2)
# x(logx)^1 ( 1 - 1/(logx))


------------
http://go.helms-net.de/math/binomial/zeta_bernoulli.pdf

  
 x = (-1/c)^(1/k) * k/(alogc)

 x = (-1/c)^(1/k) * k/ (logc * (1 - k(logc)^-1)^(1/k) )

 x = (k/logc) * (-1/c)^(1/k) * (1 - k(logc)^-1)^(1/k) 

 x = (k/logc) * (-1/c)^(1/k) * (1 - k/(logc))^(1/k) 

 x = (k/logc) * ( (-1/c) * (1 - k/(logc)) )^(1/k) 

--------------------------------

I know I'm not as knowledgeable as other folks here, but from an Architecture/SDLC perspective, I would like to through some light.

An ML application deployment has roughly 3 pipelines - Application, Model and Data pre-processing. All three areas can progress parallely and be deployed through seperate paths or pipelines handled by seperate roles.

So, it appears that you are considering the only one of these pipelines - the Model pipeline. But it has very close dependencies on other pipelines as well (for example data pre-processing or feature engineering).

* Sounds more like a Testing/verification role
* An AI application may have larger context (with APIs, user role etc) than the AI model. Are we considering the full application context?
* Just making the data available or writing data cleaning code? This role could probably go into the feature pipeline and have a lot more to do.
* This area is evolving into "no-code" kind of style, as in other areas of modern software dev practices. Once the hyper-parameters, clean data, feature repo and XPU/clusters are made availabe, the frameworks are capable of producing a package model artifact ready for deployment, that might even have a ready-made API wrapper. However tuning of hyper-parameters through bayesian optimization etc could be the focus of this role. 

Other important areas:
* Data transformations and reductions before and after the model, which are carried out by a typical fork-join jobs.
* Model verification, bench-marking by a tester role
* Model spec'ing, documentation - a very important work for keeping the model transparant
* Model monitoring - integrating the model operational metrics into a dashboard

As mentioned in the beginning, the application (API, user tokens etc) and the data processing are deployed seperately, and they might require additional roles.


